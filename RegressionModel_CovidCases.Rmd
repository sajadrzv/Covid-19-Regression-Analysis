---
title: "Regression Project_SajjadRezvani"
author: "Sajjad Rezvani"
date: "03/19/2023"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
---
---

In this Project , I am going to create a regression model to analyze number of Covid-19 cases with Socio-economic and demographic variables to determine which variables are significantly associated with the increase in number of Covid-19 cases within the various states of United States.It is worth mentioning tha data was extracted from various sources and reports, most of which were website sources data.

# 1. Importing the dataset and observing summary of our dataset

```{r}
Project=read.csv("/Users/sajjad/Desktop/Github managemenet/Project/Final files/data.csv")
Project
head(Project)
summary(Project)
```
###  As indicated above, variables are: # of covid-19 Cases,Population of each state, Number of hospitals in each state, Average winter temperature in each state, Average summer temperature in each state, Crime rate per 100,000 people in each state (Proxy for defiance from COVID-19 protocols and orders), Number of homeless people in each state, Percentage of population that are under 18 years, GDP of each state.


# 2. Data Preprocessing and developing the multiple regression model 

```{r}
sum(is.na(Project)) #checking the missing values
dat <- Project[,2:10]# Removing the "States" column

mlr <- lm(Cases ~. ,data=dat) # developing linear regression model
summary(mlr)

```

### The model depicted and adjusted R-square of 0.9821 and an F-Statistic of 337.8 with P-value of < 2.2e-16 which depicts that’s there is a significant relationship between at least one of the variables and estimated mean of the COVID-19 cases.From the above we can see the variables "Population" and "GDP" are linearly significant. 

# 3. Checking linearity assumptions 
## 3.1 Scatter plot 

```{r}
pairs(dat)
```

### After inspecting scatter plots , we found out that there is no linear relationship between response variable and some of the predictors like Winter, Summer, Crime, Homeless and Young. So,it might be solved by a transformation.

## 3.2 Checking for zero mean and constant variance of random error
 
```{r}
plot(mlr$fitted.values, mlr$residuals, 
     main = "Check for 0 mean and constant var \n  Residual vs. fitted value")
abline(h=0)
```

### As we can see in the above plot of residuals across the fitted values, in each vertical strip the average of residuals are not zero and a funnel like pattern can be seen as well. It proves that linearity assumption is violated and it is not satisfied. 


## 3.3 Checking for normality of random error by QQ plot, and Shapiro test
 
```{r}
qqnorm(mlr$residuals)
qqline(mlr$residuals)

hist(mlr$residuals)
shapiro.test(mlr$residuals)
```
### From QQ plot, we obseve several observations are far from the straight line that indicates a heavy tail. These departures also prove that residuals don't follow a normal distribution. Also by checking Shapiro test p-value = 0.02526 based on alpha=0.05 significance level , we reject null hypothesis meaning that residuals don't follow a normal distribution.


## 3.4 Checking for correlation and variance inflation factor (VIF),multicollinearity
 
```{r}
library(car)
library(carData)
cor(Project[,-1])# removing the states column
vif(mlr)
```

### From above we can easily see some of the variables are highly correlated like GDP and Population.Based on VIF also , when it is greater than 4 , we can asy that our model suffers from multicollinearity and we need to take some actions like removing insignifcant variables or applying transformation. 


## 3.5 Checking for independence of random error terms and "summer"&"winter" variables
 
```{r}
row_num <- c(1:nrow(dat))
sort_x1 <- sort(dat$Summer, index.return=TRUE)
plot(row_num, mlr$residuals[sort_x1$ix], 
     main = "Check for independence \n Residuals sorted by Summer")
abline(h=0)

# winter
sort_x2 <- sort(dat$Winter, index.return=TRUE)
plot(row_num, mlr$residuals[sort_x2$ix], 
     main = "Check for independence \n Residuals sorted by Winter")
abline(h=0)
```
### Another assumption we need to prove is that error terms are indepedent while considering the above plots, a pattern can be seen and average of residuals in each vertical stripe is not zero that shows our linearity assumption is not satisfied and there is no linear relationship between the regressors and response variable.The next step is to get transformation to solve this problem.


# 4. Transformation

```{r}
pairs(dat$Cases ~ log(dat$Summer)+log(dat$Winter)+log(dat$Crime)+log(dat$Homeless)+log(dat$Young))
```

## We transformed the dataset by taking the log of the following independent variables: Winter, summer, Crime, Homeless and Young. This transformation made the relationship between the transformed variables and dependent variable approximately linear.So,from now on we use these trasnformed data. 


# 5. Working with new transformed dataset for fitting new MLR and checking correlations and VIF
```{r}
dat_tran <- read.csv("/Users/sajjad/Desktop/Github managemenet/Project/Final files/data_transformed.csv")

dat2<- dat_tran[,2:10]
head(dat2)

# fitting new mlr model by transformed data
mlr2 <- lm(Cases ~. , data=dat2)
summary(mlr2)
vif(mlr2)
cor(dat2)
```

## In this new mlr model, the vif assigned to some variables like GDP ,Hospitals and Population is greater than 4 that indicates this model still suffers from multicollinearity. Also, some variables are highly correlated with each other that makes us to remove some of these varaibles from our model based on their significancy level.We removed the following variables: Hospitals,Log_Crime,and Log_homeless



# 6. Corrections and eliminating some varaiables from our dataset

```{r}
dat_c3 <- subset(dat2,select = -c(Hospitals,Log_Crime,Log_homeless,GDP)) # removing the columns Hospitals,Log_Crime,and Log_homeless and GDP
mlr_c3 <- lm(Cases ~. , data = dat_c3) # fitting new model
summary(mlr_c3)
vif(mlr_c3)
cor(dat_c3)
```


## In this new mlr model, vif is no longer an issue. Given that our model doesn't suffers from multicollinearity.


# 7. Variable Selection using Stepwise bidirection on the previous model we built(metric: adjusted Rsquare)

```{r}
library(StepReg)
stepwise(dat_c3, y="Cases", selection = "bidirection", select = "adjRsq")
mlr_bid <- lm(Cases ~ Log_summer + Population, data = dat_c3)
summary(mlr_bid)
```

## Based on the stepwise bidirection variable selection , selected variables are Population and Log_summer.We fit our new model considering these two variables.



# 8. Checking Linearity assumptions for the new fitted model (mlr_bid)

```{r}
# Scatter plot
pairs(dat_c3)

# Checking zero mean and constant variance
plot(mlr_bid$fitted.values, mlr_bid$residuals, 
     main = "Check for 0 mean and constant var \n  Residual vs. fitted value")
abline(h=0)

# Checking for independence of random error
par(mfcol = c(2, 1))
row_num <- c(1:nrow(dat))
sort_x1 <- sort(dat_c3$Log_summer, index.return=TRUE)
plot(row_num, mlr_bid$residuals[sort_x1$ix], 
     main = "Check for independence \n Residuals sorted by Winter")
abline(h=0)
sort_x2 <- sort(dat_c3$Population, index.return=TRUE)
plot(row_num, mlr_bid$residuals[sort_x2$ix], 
     main = "Check for independence \n Residuals sorted by Summer")
abline(h=0)


# Checking for normality of random error, Shapiro test 
qqnorm(mlr_bid$residuals)
qqline(mlr_bid$residuals)

hist(mlr_bid$residuals)
shapiro.test(mlr_bid$residuals)


```

## After checking linearity assumptions for the new fitted model(mlr_bid) ,almost all the assumptions are satisfied .Just in QQ plot , we see some departures from straight line that might be an issue that is proved also by shapiro test indicating that residuals don't follow a complete normal distribution. So ,for solving this problem we want to discover the outliers and remove them.


# 9. Identifying influential points , 2 predictors and 50 observations (k= 2, n=50)

## 9.1 Finding High leverage Points using Hat-matrix method  

```{r}
# hat values
hv <- hatvalues(mlr_bid)
k= 2
n=50
which(hv > (2*(k+1)/n))
row_num <- c(1:n)
plot(row_num, hv, xlab = "row number",
     ylab = "hatvalues", 
     main = "Identification of high leverage points")
abline(h = (2*(k+1)/n))

```


## 9.2 Finding Outliers using externally standardized residuals method 

```{r}
# Use externally standardized residuals (rstudent)
r2 <- rstudent(mlr_bid)
which(abs(r2) > 3)
row_num <- c(1:n)
plot(c(1,n), c(-6, 6), xlab = "row number",
     ylab = "externally studentized residuals", 
     main = "Identification of outlier points",
     type = "n")
points(row_num, r2)
abline(h = 3)
abline(h = -3)

```

## 9.3 Finding Outliers using COVRATIO method  

```{r}
# cov ratio
cv <- covratio(mlr_bid) #influential if covratio > 1 + 3*(k+1)/n OR covratio < 1 - 3*(k+1)/n
which(cv > (1 + 3*(k+1)/n))
which(cv < (1 - 3*(k+1)/n))
row_num <- c(1:n)
plot(row_num, cv, xlab = "row number",
     ylab = "covratio", 
     main = "Identification of influential points")
abline(h = (1 + 3*(k+1)/n))
abline(h = (1 - 3*(k+1)/n))


```

## 9.4 Finding Outliers using difference in fits method   

```{r}
# diffits method
dffits(mlr_bid) #influential if abs(dffits) > 2*sqrt((k+2)/(n-k-2))
which(abs(dffits(mlr_bid)) > (2*sqrt((k+2)/(n-k-2)))) # 

```

## 8.5 Finding Outliers using Cook’s distance method  

```{r}
# Cook's distance method
cooks.distance(mlr_bid) #influential if d > 1
which(cooks.distance(mlr_bid) > 1) # 

```


### Using the hat-matrix method to find high leverage points, observations 2, 5, 9 and 43 were found to have high leverage points. Using the externally standardized residuals method to check for outliers, observations 5 and 9 were found to be outliers. Also, using the COVRATIO method showed observation 5 and 9 as being influential, using the difference in fits method showed observations 2, 5, 9 and 43 to be influential and also using the Cook’s distance method showed observation 5 to be influential. After comparing the various influential points, we remove observations 2, 5 and 9 which corresponds to Alaska, California and Florida.We considered these 3 observations as influential points. 

# 10. Removing outliers and developing Final mlr model

```{r}
dat_adj=dat_c3[-c(2,5,9),]
mlr_bid_adj <- lm(Cases ~ Log_summer + Population, data = dat_adj)
summary(mlr_bid_adj)
shapiro.test(mlr_bid_adj$residuals)

```

## This new regression model will satisfy all the linearity assumptions.Adjusted R-square = 0.9781 and **Log_summer** and **Population**  are found to be significantly related to the response variable.

## Our model revealed a positive relationship between the two variables (i.e.  Logarithm of summer and population) and COVID 19 cases in the USA. This shows that as summer and population increase in a particular state, the number of COVID 19 cases is expected to increase as well.  



<hr style=" height:4px; width:90%; border:3px solid red; border-radius:12px; background-color: blue">